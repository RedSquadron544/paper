\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}

\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\newcommand{\code}[1]{\texttt{#1}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Libert\'{e}, \'{E}galit\'{e}, Fraternit\'{e} \\ Stance Detection in French Language Tweets}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Michael Baltz: 6231300032, \code{mbaltz@usc.edu}
	Rachita Pradeep: 4891102449, \code{rachitap@usc.edu} \\
	Stephen Magro: 4937098659, \code{smagro@usc.edu} 
Ted Monyak: 2537595042, \code{monyak@usc.edu}}
% <-this % stops a space

% The paper headers
\markboth{USC CSCI 544}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Division of Labor}
All team members worked on collecting and labeling data for the project. Stephen and Ted worked concurrently to develop and test the recurrent Neural Network. Stephen was responsible for the development of the fastText model and designing the RNN structure. Ted evaluated performance and debugged issues with this code. Michael and Rachita worked together on the word2vec implementation. Rachita gathered tweets and created the pre-computed vector models for the classification algorithm. Michael wrote the logistic regression classifier which used this model. All team members wrote similar-length segments of the final report. 
\clearpage

\section{Introduction}
One of the most talked-about stories from the 2016 U.S. Presidential Campaign was the emergence of “fake news” across the internet. The current French Presidential campaign, which will conclude in May 2017, resembles the U.S. campaign in several ways, most notably with the rise of a populist right-wing candidate in Marine Le Pen, and the prevalence of news sharing on the internet. 

Most efforts to combat fake news have been focused on English, including the system recently built using the Emergent data-set \cite{ferreira_vlachos_2016}. However, there have been few systems tailored for French, and due to the timing of their election, this is where we chose to focus. Due to the complexity of the fake news detection task, we investigated a sub-task: stance detection. Stance detection is the determination of an author’s opinion on a topic, and is a helpful first step for general fake news detectors. Our goal was to classify the opinion expressed by a body of text relative to a topic as being in favor of, against, neutral, or unrelated.

In late 2016, the Fake News Challenge was announced \cite{fake_news_challenge}, to foster the development of tools that can assist in fake news detection. The first phase of this competition is aimed at the stance detection of the relationship between an article headline and an article body. However, due to the absence of a reliable data-set for fake news articles in French, we focused on determining the stance of standalone tweets on a given topic. Twitter contains primarily opinionated bodies of text, and a high volume of sharing, so it makes an ideal target for our system. 

In this paper, we propose two methods for stance detection: a logistic classifier trained on a word2vec model, and a recurrent neural network that uses transfer learning, as inspired by an entry \cite{DBLP:journals/corr/ZarrellaM16} to the SemEval-2016 competition on stance detection, which demonstrated a successful recurrent neural network for English stance detection even with a small amount of training data.

\clearpage

\section{Materials}
A large volume of French language tweets were collected from Twitter using the Twitter Streaming API. 250 MB of tweet text was collected, which consisted of around 1.8 million tweets - many regarding the 2017 French presidential elections. The properties to create a good labeled stance data-set, is to use a target that is commonly understood by a wide range of people and get significant amount of data for all four classes - agree, disagree, neither and unrelated. The collected data was annotated for the topics "Je soutiens Marine Le Pen" ("I support Marine Le Pen"), "Je soutiens Fran\c{c}ois Fillon" ("I support Fran\c{c}ois Filon") and "Je soutiens Emmanuel Macron" ("I support Emmanuel Macron"). It is important to note that the target need not be explicitly mentioned in the tweets. The tweets that depicted favorable stance were labeled as “agree”, those that depicted unfavorable stances were labeled “disagree” and the tweets lacked evidence for favor or against were labeled "neither". A neither stance of a tweet does not indicate that the tweeter is neutral towards the target, it just means the stance could not be deduced from the tweets. Tweets that were unrelated to the target were labeled as "unrelated". Retweets and tweets which consisted of only a link to an external page were excluded from the data-set. The final labeled data consisted of 254 agree, 260 disagree, 142 neither, and 65 unrelated tweets. For this experiment we decided against uniformly distributing labeled data by class in favor of keeping the class distribution we mined from Twitter.

\clearpage

\section{Procedures}
We present two stance detection implementations - a word2vec Logistic Regression implementation and Recurrent Neural Network implementation - as well as a baseline classifier. As a general tokenization of tweets used in this experiment we used the nltk tweet tokenizer to segment our data \cite{nltk}. 

\subsection{Baseline Classifier} 
To establish performance of our various methods we developed a baseline classifier. The classifier is similar to a four class Naive Bayes classifier in which the word probability priors are calculated from the training set. For the test data, candidate probabilities are calculated for all four classes using the precomputed priors given the probability of each label for the candidate. The label with the maximum probability for a given candidate is taken as the predicted value for a tweet. Laplace smoothing was implemented for previously unseen values. 

\subsection{word2vec Logistic Regression}
The \code{word2vec} \cite{DBLP:journals/corr/abs-1301-3781} approach was informed by \cite{MOHAMMAD16.232,DBLP:conf/starsem/SobhaniMK16}.
The \code{word2vec} logistic regression model was implemented using the gensim \cite{rehurek_lrec} library, a Python library for analyzing plain-text documents for semantic structure. The model uses 4 main parameters for training: \code{size}, \code{window}, \code{min\_count} and $\alpha$. The dimensionality of the feature vectors is set to 700. \code{window} is the maximum distance between the current and the predicted word in a sentence which is set to 4. The \code{min\_count} is minimum frequency below which the word is ignored - is set to 2 which helped to remove URLs from appearing in our model. The initial learning rate, $\alpha$, is set to 0.025. The \code{word2vec} model is trained using the 250MB of French tweets and the final model consisted of around 60000 word vectors.

This model was then used to compute the cosine similarity between a tweet and a topic. A multiclass logistic regression model was trained, using the \code{scikitlearn} \cite{scikit-learn} Python library, on the the cosine similarities between the tweet and topic word vectors. This model was used to predict class labels for test data. This method was drawn for the analysis of new articles in which cosine similarity is computed between news headline and news articles, however, in our experiment we used a tweet and a topic statement and computed the cosine similarities between their respective cosine similarities. The logistic regression utilizes the \code{lbfgs} solver and a \code{multinomial} loss fit for the across the probability distribution. In the the event of a token not appearing in our vector model, this data point is treated as an error by the classification and is ignored by the cosine similarity, however, this is often only the problem for tokens with one occurrence in the entire data-set. All tweets have tokens that appear in the vector model. 

\subsection{Recurrent Neural Network}
Our final approach is a Recurrent Neural Network built using the \code{Keras} \cite{chollet2015} Python library on top of TensorFlow \cite{tensorflow2015-whitepaper}. The RNN is built using two LSTMs (one for the topic input and one for the tweet text input). The input to the topic LSTM is a string describing the topic, such as "Je soutiens Marine Le Pen" ("I support Marine Le Pen"). This allows the model to extract some meaning from the topic so that the resulting model can classify tweets for multiple topics. Before each LSTM is an embedding layer where each word in the topic/tweet is converted to a 100-dimension word vector using a fastText \cite{bojanowski2016enriching} model trained on a large corpus of French language tweets that we collected. There is also a convolutional layer that operates on the tweet embedding, that is able to extract some more information from the tweet. The outputs of the topic and tweet LSTMs and the convolutional layer are concatenated and connected directly to a densely connected layer of 4 neurons. This final layer represents a one-hot encoded class that classifies the tweet's stance. This structure was arrived at experimentally and proved to be the best performing architecture that we attempted. Other attempts included training a separate network for each topic, but that resulted in lower accuracy because there was less data for the model to learn from. The neural net was trained using a dropout rate of 0.2 on all layers to prevent the model overfitting our training data.


\clearpage


\section{Evaluation}

\subsection{Classification Evaluation}
For our results we used a k-fold cross validation scheme with k value of four, reporting the accuracy, precision, recall, and f1 for each fold as well as the average and standard deviation across all folds. These methods are common to evaluate the efficacy of supervised classification algorithms and in referenced sources similar metrics were also used. 

\subsection{Labelling Evaluation}
Our data-set was pulled tweets from the Twitter API \cite{Twitter_API} using Tweepy \cite{tweepy}, and the data was labeled manually by group members. Using this API we were able to collect tweets targeting specific topics in the French language that were tweeted from the geographical bounding box around France. For manual evaluation tweets were run through Google Translate and team members used this English translation and the context of the tweet to provided an label for each data point. Each tweet was reviewed by three team members and only tweets that had a majority consensus regarding the label were included in the final data-set. Retweets and duplicate posts were excluded from the labeled data-set to ensure that data points were unique. We considered using hashtags as a noisy labeling method, but ultimately these results were excluded from this assessment for accuracy reasons as hashtags we found were often used ironically (or simply as a way to categorize a tweet to a topic, not to take a stance) and thus are not a good indicator of stance. 




\clearpage


\section{Results}

\begin{center}
\captionof{K-Fold CV Result (k = 4)} \label{tab:title2}
\begin{tabular}{ | c | c | c | c | c |}
\hline
Metrics & Baseline & Logit Regression & RNN\\[0.7ex]
\hline
Accuracy & 48.68\% +/-2.91\% & 42.72\% +/-1.57\% & 50.15\% +/-5.80\% \\
Precision & 54.07\% +/-3.66\% & 35.20\% +/-4.44\% & 52.79\% +/-7.52\% \\
Recall & 48.68\% +/-2.91\% & 42.72\% +/-1.57\% & 35.31\% +/-6.60\% \\
F1 Value & 0.46 +/-0.03 & 0.36 +/-0.01 & 0.45 +/-0.07 \\ [1ex]
\hline
\end{tabular}
\end{center}

Classification of "Macron est le pire pr\'{e}sident pour la France" ("Macron is the worst president for France") \\ on the topic "Je soutiens Emmanuel Macron" ("I support Emmanuel Macron")
\begin{center}
\begin{tabular}{ | c | c | c | c | c |}
\hline
Method & Agree & Disagree & Neither & Unrelated \\[0.7ex]
\hline
Baseline & 3.2e-20 & \textbf{1.4e-20} & 2.1e-19 & 3.7e-20\\
word2vec & N/A & \textbf{N/A} & N/A & N/A \\
RNN & 0.19 & \textbf{0.32} & 0.30 & 0.18 \\
\hline
\end{tabular}
\end{center}


Of the methods presented in this paper the results from the recurrent neural network had the best accuracy for tweet stance detection. This result is consistent with trends from SemEval 2016 \cite{DBLP:journals/corr/ZarrellaM16} in which many submissions for a similar stance detection task on English tweets used neural networks. The logistic regression technique drew inspiration from work done with the Emergent Info data-set \cite{ferreira_vlachos_2016}, however, this paper applied this algorithm to stance detection in new articles achieving an accuracy of 73\%. We attribute the discrepancy in our results for this method to the shorter lengths of our data points, as tweets are limited to 140 characters. This means for a tweet there were fewer word vectors with which to compute the cosine similarity than the average news article in the Emergent Info data-set \cite{ferreira_vlachos_2016}. The baseline classifier slightly outperformed the word2vec classification method. 

\clearpage
\section{Discussion}

Inspired by similar research and systems using English data \cite{DBLP:journals/corr/ZarrellaM16}, we have shown that a recurrent neural network using transfer learning is the most effective method out of the three that we proposed at stance detection on French tweets. Therefore, we have shown that this methodology translates well between languages, an important finding in the international efforts to combat fake news independent of language. 

The word2vec model was limited in its success, we believe, due to the brevity of individual tweets. We hypothesize that the use of the Emergent data-set \cite{ferreira_vlachos_2016} was more effective due to the longer word vectors in entire article bodies. It did not surpass the performance of our baseline model, which employs a Naive Bayes implementation to classify stance. However, the baseline performance was still somewhat low, and we hypothesize that this once again has to do with the brevity of tweets creating sparse word probability models.

Due to the manual nature of our annotation methodology, we were limited in the amount of data we could train our models on. With more time and resources, we hope to automate our system for annotating tweets to build a larger corpus. Furthermore, in future research, we intend to adapt our system so it may be used on headline and article body pairs, which was the original problem described in phase one of the Fake News Challenge. This adaptation would make our system most useful for fake news detection, which could enable it to help combat the fake news problem in future French elections. This adds another layer of complexity to the research, in compiling a sizeable French data-set such as the Emergent data-set \cite{ferreira_vlachos_2016}. 



\clearpage
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{unsrt}%Used BibTeX style is unsrt


\bibliography{references}



\end{document}


